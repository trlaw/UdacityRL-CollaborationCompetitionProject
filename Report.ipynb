{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fb1839",
   "metadata": {},
   "source": [
    "# Collaboration and Competition Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1e55",
   "metadata": {},
   "source": [
    "![Screenshot of tennis environment](doc/BannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a037",
   "metadata": {},
   "source": [
    "This is an implementation of the Deep Deterministic Policy Gradients Algorithm for training a agent to play tennis.  The environment contains two agents.  Each agent has a continuous action space corresponding horizontal and vertical paddle motion.  Each agent's observation space consists of the position and velocity of the ball and its racket.  An agent is assigned a reward of +0.1 each time it hits the ball over the net, and a reward of -0.01 if it hits the ball out of bounds, or lets the ball drop on its side of the play area.\n",
    "\n",
    "To solve the environment, a running average, over 100 consecutive episodes, of the maximum score between the two agents, must be at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24340d60",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - N-Step Bootstrapping\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Helpers\n",
    "  - Network Definition\n",
    "  - Training Code\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbce7a5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bcadd",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file into any convenient directory, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2868f93",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65116bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "    \"C:/Projects/UdacityRLp3/Tennis_Windows_x86_64/Tennis_Windows_x86_64/Tennis.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ffd90",
   "metadata": {},
   "source": [
    "## Description of Algorithm\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm extends the application of Q-Learning methods to action spaces with continuously valued dimensions [[1]](#References).  There are two networks involved, a Policy Network and Action Value (Q) network.  During learning, the Policy Network generates an action according to the input state, and both this action and the state are supplied to the Q network as input.  The Q network outputs a single action value, and the gradient of this value with respect to the parameters of the Policy Network are used to nudge the policy towards one with a higher action value (by gradient ascent). <br><br> \n",
    "In typical Deep Q Learning (DQN), each action has a corresponding output from the Q network, but this representation of the Action Value Function $Q(s,a)$ does not naturally accomodate continuously-valued actions.  The primary difference of DDPG with respect to DQN, is that actions are instead explicit, continuously valued inputs to the Action Value function approximation.  This allows closed-form computation of the gradient of $Q(s,a)$ with respect to changes in magnitudes of the continuously-valued action variables.\n",
    "\n",
    "The basic DDPG algorithm reads as follows (from [[1]](#References)): <br>\n",
    "![DDPG Algorithm](doc/DDPG_alg.png) <br>\n",
    "\n",
    "This implementation does not require that the environment steps and learning steps happen at the same time, or in a 1:1 ratio.  It also incorporates the following improvements:\n",
    "\n",
    "### N-Step Bootstrapping\n",
    "\n",
    "The hyperparameter `n_step_order`, determines the value of $n$ in the following alternative Bellman Update target, replacing the definition for $y_i$ in the algorithm above:\n",
    "\n",
    "$$y_i = r_i + \\gamma r_{i+1} + \\gamma^2 r_{i+2} + ... + \\gamma^n r_{i+n} + \\gamma^{n+1} Q'(s_{i+n+1},\\mu '(s_{i+n+1}|\\theta^{\\mu '})|\\theta^{Q'})$$\n",
    "\n",
    "N-Step Bootstrapping increases the relative weight of sampled rewards from the environment, compared to rewards estimated by the Action Value function $Q(s,a)$. Anecdotally, this seems to assist action values propagating backwards in time and through 'bottlenecks' where most nearby states have comparatively low State Values.  So essentially, initial learning can be faster, and some connections may be made that would otherwise take an unacceptably long time to be made without N-Step Bootstrapping.  However, real rewards are stochastic, and an atypically bad or good run of events will  more readily propagate through a Q network with N-Step Bootstrapping.  If an agent quickly changes its behavior between simple, regimented approaches, it is possible the `n_step_order` value in use is too high for the agent's environment.\n",
    "\n",
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameters of the Policy and Q networks.  Like in [[2]](#References), the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\alpha}, \\alpha \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\alpha$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [[2]](#References).\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed.\n",
    "\n",
    "### Reward Shaping\n",
    "The signal for failure seemed to require an excessive number of examples to change agent behavior, so a hyperparameter `rwd_mult_neg` was added to allow increasing of the magnitude of the negative reward relative to the positive 'over the net' reward.\n",
    "\n",
    "### Persistent Experience Storage and Retrieval\n",
    "Since the training time is dominated by the 'real-time' rate of the Tennis environment's simulation, the functionality to store all experiences from a training run to a file was added.  Further, the implementation allows the experience replay buffer of a new training run to be pre-populated by the experiences from a provided file list.  This enables iteration over different hyperparameter choices much faster, since a new agent can be trained without having to accumulate 500 ball drop experiences every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4222faf",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Environment\n",
    "`state_dim`: Dimension of the observable state space<br>\n",
    "`act_dim`: Dimension of the action space for each agent <br>\n",
    "`num_agnt`: Number of agents in the environment\n",
    "\n",
    "#### Network Models\n",
    "`pol_hid_num`: Number of hidden layers in the Policy Network<br>\n",
    "`pol_hid_size`: Number of neurons in each hidden layer of the Policy Network<br>\n",
    "`noise_init`: Initial noise magnitude for Orstein-Uhlenbeck noise applied to policy<br>\n",
    "`noise_decay`: Noise magnitude is multiplied by this factor after each episode<br>\n",
    "`noise_sigma`: Magnitude of white noise input to Orstein-Uhlenbeck, see [[1]](#References)<br>\n",
    "`noise_theta`: Mean-reversion constant for Orstein-Uhlenbeck, see [[1]](#References)<br>\n",
    "`q_hid_num`: Number of hidden layers in the Q Network<br>\n",
    "`q_hid_size`: Number of neurons in each hidden layer of the Q Network<br>\n",
    "\n",
    "#### Reward Parameters\n",
    "`gamma`: Discount factor per step for rewards<br>\n",
    "`n_step_order`: Number of reward steps to directly incorporate into Bellman Update estimate<br>\n",
    "\n",
    "#### Replay Parameters\n",
    "`buf_life`: Buffer will be reset every this many `sample()` calls to replay buffer<br>\n",
    "`buf_min_size`: Learning will not be allowed unless replay buffer has this many experiences, to avoid overfitting<br>\n",
    "`alpha`: Prioritization strength factor, see [[2]](#References)<br>\n",
    "`beta`: Importance sampling correction coefficient, see [[2]](#References)<br>\n",
    "\n",
    "#### Optimization Parameters\n",
    "`pol_lr`: Learning rate for Policy Network optimizer<br>\n",
    "`q_lr`: Learning rate for Q Network optimizer<br>\n",
    "`lr_int`: Number of environment episodes between each learning phase<br>\n",
    "`lr_stps`: How many learning steps are applied during each learning phase<br>\n",
    "`batch_size`: How many experiences are processed by each agent for each learning step<br>\n",
    "`p_tau`: Soft update factor for target Policy Network, applied once every learning step<br>\n",
    "`q_tau`: Soft update factor for the Q Network, applied once every learning step<br>\n",
    "`rwd_mult_neg`: Multiplier applied to negative rewards issued by the environment before they are incorporated into the experience replay buffer<br>\n",
    "`pol_upd_int`: Updated policy network only this often relative to q-network update<br>\n",
    "\n",
    "#### Training Parameters\n",
    "`max_eps`: Maximum number of episodes for which to train<br>\n",
    "`avg_wnd_len`: Length (in episodes) of running average buffer for reported performance<br>\n",
    "`rprt_int`: Number of episodes between prints of performance<br>\n",
    "`slv_thresh`: Minimum average score constituting solution of environment, the achievement of which will end the training run<br>\n",
    "`starting_exps_paths`: Python list of paths containing experiences saved from previous training runs.  Each file in the list will have its experiences loaded into the experience replay buffer before starting the training run.<br>\n",
    "`output_exp_path`: New experiences generated during the training run will be saved to this file path at the run's conclusion.<br>\n",
    "`random_policy`: If set to `True`, the policy neural network will be ignored when selecting environment actions, and actions will be completely determined by the output of the noise generator.  Potentially useful for getting unbiased initial experiences from which to start other training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf8986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Hyperparameters():\n",
    "    def __init__(self,\n",
    "                 state_dim=24,\n",
    "                 act_dim=2,\n",
    "                 num_agnt=2,\n",
    "                 pol_hid_num=1,\n",
    "                 pol_hid_size=100,\n",
    "                 noise_init=1.0,\n",
    "                 noise_decay=0.99,\n",
    "                 noise_theta=0.1,\n",
    "                 noise_sigma=0.1,\n",
    "                 q_hid_num=1,\n",
    "                 q_hid_size=100,\n",
    "                 gamma=0.9,\n",
    "                 n_step_order=1,\n",
    "                 buf_life=1e7,\n",
    "                 buf_min_size=512,\n",
    "                 alpha=0.6,\n",
    "                 beta=1.0,\n",
    "                 pol_lr=1e-4,\n",
    "                 q_lr=1e-3,\n",
    "                 lr_int=10,\n",
    "                 lr_stps=100,\n",
    "                 batch_size=64,\n",
    "                 p_tau=1e-3,\n",
    "                 q_tau=1e-3,\n",
    "                 max_eps=1000000,\n",
    "                 avg_wnd_len=100,\n",
    "                 rprt_int=100,\n",
    "                 slv_thresh=0.5,\n",
    "                 rwd_mult_neg=1.0,\n",
    "                 pol_upd_int=1,\n",
    "                 starting_exps_paths = None,\n",
    "                 output_exp_path = 'exp_output.pkl',\n",
    "                 random_policy = False\n",
    "                 ):\n",
    "        self.state_dim, self.act_dim, self.num_agnt = state_dim, act_dim, num_agnt\n",
    "        self.pol_hid_num, self.pol_hid_size, self.noise_init = pol_hid_num, pol_hid_size, noise_init\n",
    "        self.q_hid_num, self.q_hid_size, self.gamma = q_hid_num, q_hid_size, gamma\n",
    "        self.n_step_order, self.buf_life, self.buf_min_size = n_step_order, buf_life, buf_min_size\n",
    "        self.alpha, self.beta, self.pol_lr = alpha, beta, pol_lr\n",
    "        self.q_lr, self.lr_int, self.lr_stps = q_lr, lr_int, lr_stps\n",
    "        self.batch_size, self.p_tau, self.q_tau = batch_size, p_tau, q_tau\n",
    "        self.max_eps, self.avg_wnd_len, self.rprt_int = max_eps, avg_wnd_len, rprt_int\n",
    "        self.slv_thresh, self.noise_decay, self.noise_theta = slv_thresh, noise_decay, noise_theta\n",
    "        self.rwd_mult_neg, self.pol_upd_int, self.noise_sigma = rwd_mult_neg, pol_upd_int, noise_sigma\n",
    "        self.starting_exps_paths, self.output_exp_path = starting_exps_paths, output_exp_path\n",
    "        self.random_policy = random_policy\n",
    "\n",
    "def_hyp = DDPG_Hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7faf4",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a675fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging function\n",
    "def tensor_check(input,desc,exp_size):\n",
    "    if torch.any(torch.isnan(input)):\n",
    "        print(f'NaNs in {desc}:')\n",
    "        print(input)\n",
    "    if torch.any(torch.isinf(input)):\n",
    "        print(f'Inf in {desc}:')\n",
    "        print(input)\n",
    "    if not (input.size() == torch.Size(exp_size)):\n",
    "        print(f'{desc} has size {tuple(input.size())}, not {exp_size} expected')\n",
    "\n",
    "# Copied from the Lunar Lander dqn_agent.py file of the Udacity repo for course\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "# Object that represents an experience in the experience buffer or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "# Based on code in Reference [3]\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "        \n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "        \n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,hyp):\n",
    "        self.buf_life = hyp.buf_life\n",
    "        self.alpha = hyp.alpha\n",
    "        self.store = SumTreeNode()\n",
    "        self.sample_count = 0\n",
    "        self.exp_count = 0\n",
    "        self.beta = hyp.beta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.exp_count\n",
    "    \n",
    "    def add_experience(self, experience, loss):\n",
    "        new_p_i = pow(loss, self.alpha)\n",
    "        self.store.attach_child(SumTreeNode(experience, new_p_i))\n",
    "        self.exp_count += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        samples = ([self.store.weighted_retrieve(p_samp) for p_samp in sample_keys],\n",
    "                   (self.exp_count,self.store.p_i))\n",
    "        self.sample_count += 1\n",
    "        if (self.sample_count >= self.buf_life):\n",
    "            self.sample_count = 0\n",
    "            self.store = SumTreeNode()\n",
    "            self.exp_count = 0\n",
    "            print ('\\nFlushed replay buffer!\\n')\n",
    "        return samples\n",
    "    \n",
    "# Circular buffer for generation of n_step rewards\n",
    "class MultistepBuffer():\n",
    "    def __init__(self,hyp):\n",
    "        self.n_step_order = hyp.n_step_order\n",
    "        self.store = deque(maxlen = hyp.n_step_order + 1)\n",
    "        self.gamma = hyp.gamma\n",
    "    \n",
    "    def add_experience(self,exp):\n",
    "        self.store.append(exp)\n",
    "    \n",
    "    def ready(self):\n",
    "        return len(self.store) == (self.n_step_order + 1)\n",
    "    \n",
    "    def get_n_step_experience(self):\n",
    "        out_state = self.store[0].state\n",
    "        out_action = self.store[0].action\n",
    "        out_reward = \\\n",
    "            sum([((self.store[i].reward) * pow(self.gamma,i)) for i in range(self.n_step_order)])\n",
    "        out_final_state = self.store[-1].state\n",
    "        return SumTreeNode(Experience(out_state, out_action, out_reward, out_final_state),p_i=1)\n",
    "    \n",
    "# Logger for running average\n",
    "class PerformanceLogger():\n",
    "    def __init__(self,avg_wnd_len=100,starting_scores=None):\n",
    "        self.avg_wnd_len = avg_wnd_len\n",
    "        self.scores = starting_scores if starting_scores is not None else []\n",
    "        self.internal_run_avg = 0\n",
    "        \n",
    "    def add_score(self,score):\n",
    "        self.scores.append(score)\n",
    "        self.internal_run_avg += score / self.avg_wnd_len\n",
    "        # Remove tail of running average\n",
    "        if len(self.scores) > self.avg_wnd_len:\n",
    "            self.internal_run_avg -= self.scores[-(self.avg_wnd_len + 1)] / self.avg_wnd_len\n",
    "    \n",
    "    def has_full_window(self):\n",
    "        return len(self.scores) >= self.avg_wnd_len\n",
    "    \n",
    "    def run_avg(self):\n",
    "        if len(self.scores) == 0:\n",
    "            return 0\n",
    "        return self.internal_run_avg \\\n",
    "                * (1 if self.has_full_window() else (self.avg_wnd_len/len(self.scores)))\n",
    "    \n",
    "# Ohrstein-Uhlenbeck Process Noise Generator\n",
    "class OhrsteinUhlenbeckGen():\n",
    "    def __init__(self,out_dim=1,theta=1.0,sigma=1.0):\n",
    "        self.state = torch.zeros(out_dim,dtype=torch.float32)\n",
    "        self.out_dim = out_dim\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = torch.zeros(self.out_dim,dtype=torch.float32)\n",
    "        \n",
    "    def sample(self):\n",
    "        step_noise = self.sigma * \\\n",
    "                         torch.clamp(torch.randn(self.out_dim,dtype=torch.float32),-5.0,5.0)\n",
    "        self.state = ((1.0 - self.theta) * self.state) + step_noise\n",
    "        return step_noise\n",
    "    \n",
    "# Aggregate saved experiences and metadata\n",
    "ExperienceSet = namedtuple('ExperienceSet',['hyperparams','exps'])\n",
    "\n",
    "# Save list of experiences\n",
    "def save_experiences(hyp,exp_list):\n",
    "    torch.save(ExperienceSet(hyp,exp_list),hyp.output_exp_path)\n",
    "    print(f'Saved {len(exp_list)} experiences to {hyp.output_exp_path}')\n",
    "\n",
    "# Initialize replay buffer\n",
    "def load_experiences(hyp):\n",
    "    out_buf = PrioritizedReplayBuffer(hyp)\n",
    "    if hyp.starting_exps_paths is None:\n",
    "        return out_buf\n",
    "    for exp_set_path in hyp.starting_exps_paths:\n",
    "        exp_set = torch.load(exp_set_path)\n",
    "        for exp in exp_set.exps:\n",
    "            out_buf.add_experience(exp,1.0)\n",
    "    print(f'\\nReplay buffer created with initial size {len(out_buf)}')\n",
    "    return out_buf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62594cf0",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff21130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic MLP\n",
    "class DDPG_Subnet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_dim, \n",
    "                 out_dim, \n",
    "                 hid_size, \n",
    "                 num_hid,\n",
    "                 squish_output=False\n",
    "                 ):\n",
    "        super(DDPG_Subnet,self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_dim, hid_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(num_hid-1):\n",
    "            layers.append(nn.Linear(hid_size, hid_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hid_size, out_dim))\n",
    "        self.reg_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.squish_output = squish_output\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.float()\n",
    "        x = self.reg_layers(x)\n",
    "        if self.squish_output:\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Generators\n",
    "def new_pol_net(hyp):\n",
    "    return DDPG_Subnet(hyp.state_dim*hyp.num_agnt,\n",
    "                       hyp.act_dim*hyp.num_agnt,\n",
    "                       hyp.pol_hid_size,\n",
    "                       hyp.pol_hid_num,\n",
    "                       True)\n",
    "                       \n",
    "def new_q_net(hyp):\n",
    "    return DDPG_Subnet((hyp.state_dim + hyp.act_dim)*hyp.num_agnt,\n",
    "                       1,\n",
    "                       hyp.q_hid_size,\n",
    "                       hyp.q_hid_num,\n",
    "                       False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabdd11",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3caa249",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`q_net`: Q network to use for training run<br>\n",
    "`pol_net`: Policy network to use for training run<br>\n",
    "`env`: Environment to use for training run<br>\n",
    "`hyp`: `DDPG_Hyperparameters` object to use for training run<br>\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to a `PerformanceLogger` with the score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020d778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to compute importance sampling weight corrections\n",
    "def comp_w_i(experiences, hyp, device):\n",
    "    \n",
    "    p_i_total = experiences[1][1]\n",
    "    buf_N = experiences[1][0]\n",
    "    # Normalized p_i see eqn (1) of reference [3]\n",
    "    p_i = torch.tensor([(e.p_i/p_i_total) for e in experiences[0]]).to(device)\n",
    "    p_i = torch.clamp(p_i,min=1e-9)\n",
    "    w_i = torch.pow(torch.reciprocal(torch.mul(buf_N,p_i)),hyp.beta).unsqueeze(dim=1)\n",
    "    w_i = torch.clamp(w_i,1e-8,1.0)\n",
    "    \n",
    "    return w_i\n",
    "\n",
    "def Q_Loss(q_net, q_trg, pol_trg, experiences, hyp, device):\n",
    "    \n",
    "    init_states = torch.vstack([e.data.state for e in experiences[0]]).to(device)\n",
    "    actions = torch.vstack([e.data.action for e in experiences[0]]).to(device)\n",
    "    rewards = torch.tensor([e.data.reward for e in experiences[0]]).unsqueeze(dim=1).to(device)\n",
    "    final_states = torch.vstack([e.data.last_state for e in experiences[0]]).to(device)\n",
    "\n",
    "    q_net_input = torch.cat((init_states,actions),dim=1).float()\n",
    "    #tensor_check(q_net_input,'Q Loss q_net_input',(len(experiences[0]),hyp.state_dim + hyp.act_dim))\n",
    "    q_net_output = q_net(q_net_input)\n",
    "    \n",
    "    q_trg_input = torch.cat((final_states,pol_trg(final_states)),dim=1).float()\n",
    "    #tensor_check(q_trg_input,'q_trg_input',(len(experiences[0]),hyp.state_dim + hyp.act_dim))\n",
    "    q_trg_output = q_trg(q_trg_input)\n",
    "    \n",
    "    disc = pow(hyp.gamma,hyp.n_step_order)\n",
    "    #tensor_check(rewards,'rewards',(len(experiences[0]),1))\n",
    "    q_loss = (rewards + (disc * q_trg_output)) - q_net_output\n",
    "    q_loss = torch.pow(q_loss,2)\n",
    "    #tensor_check(q_loss,'q_loss',(len(experiences[0]),1))\n",
    "    \n",
    "    # Importance sampling weights\n",
    "    w_i = comp_w_i(experiences, hyp, device)\n",
    "    #tensor_check(w_i,'w_i',(len(experiences[0]),1))\n",
    "    \n",
    "    return (torch.mul(q_loss,w_i), w_i)\n",
    "    \n",
    "def Pol_Loss(q_net, pol_net, experiences, hyp, device):\n",
    "    \n",
    "    init_states = torch.vstack([e.data.state for e in experiences[0]]).to(device)\n",
    "    w_i = comp_w_i(experiences, hyp, device)\n",
    "    q_net_input = torch.cat((init_states,pol_net(init_states)),dim=1).float()\n",
    "    #tensor_check(q_net_input,'Pol Loss q_net_input',(len(experiences[0]),hyp.state_dim+hyp.act_dim))\n",
    "    q_net_output = q_net(q_net_input)\n",
    "    weighted_output = torch.mul(q_net_output,w_i)\n",
    "    tensor_check(weighted_output,'weighted_output',(len(experiences[0]),1))\n",
    "    out_loss = torch.mul(-1.0,torch.mean(weighted_output))\n",
    "    #tensor_check(out_loss,'out_loss',())\n",
    "    return out_loss\n",
    "\n",
    "# Processing of completed n-step experiences\n",
    "def process_new_exp(hyp, device, out_exp_list, n_step_buf, replay_buffer, q_net, q_trg, pol_trg):\n",
    "\n",
    "    if n_step_buf.ready():        \n",
    "                \n",
    "        n_step_exp = n_step_buf.get_n_step_experience()\n",
    "\n",
    "        # Compute losses to determine sampling priorities, add new states to replay buffer\n",
    "        q_net.eval(); q_trg.eval(); pol_trg.eval()\n",
    "        loss, w_i = Q_Loss(q_net, q_trg, pol_trg, \n",
    "                           ([n_step_exp],(1,1)), \n",
    "                           hyp, device)\n",
    "        pol_trg.train(); q_trg.train(); q_net.train()\n",
    "        p_i = torch.div(loss,w_i).squeeze(dim=1)\n",
    "        replay_buffer.add_experience(n_step_exp.data,p_i.item())\n",
    "        out_exp_list.append(n_step_exp.data)\n",
    "\n",
    "def train_net(pol_net, q_net, env, hyp):\n",
    "    \n",
    "    # Use gpu if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Move primary models to device\n",
    "    pol_net = pol_net.to(device)\n",
    "    q_net = q_net.to(device)\n",
    "    \n",
    "    # Generate target policy and q networks\n",
    "    pol_trg = new_pol_net(hyp).to(device)\n",
    "    q_trg = new_q_net(hyp).to(device)\n",
    "    \n",
    "    # Initialize target networks, copy policy, zeroed q network\n",
    "    soft_update(pol_net, pol_trg, 1.0)\n",
    "    soft_update(q_net, q_trg, 1.0)\n",
    "        \n",
    "    # Setup optimizers\n",
    "    pol_optim = Adam(pol_net.parameters(), lr=hyp.pol_lr)\n",
    "    q_optim = Adam(q_net.parameters(), lr=hyp.q_lr)\n",
    "    \n",
    "    # Init counters\n",
    "    lrn_cntr = 0\n",
    "    pol_upd_cntr = 0\n",
    "    rpt_cntr = 0\n",
    "    \n",
    "    # Unity ML-Agents Setup\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    # Initialize replay buffer, performance log, experience list for saving\n",
    "    replay_buffer = load_experiences(hyp)\n",
    "    perf_log = PerformanceLogger(avg_wnd_len = hyp.avg_wnd_len)\n",
    "    out_exps = []\n",
    "    \n",
    "    # Noise generator\n",
    "    noise_gen = OhrsteinUhlenbeckGen(hyp.act_dim*hyp.num_agnt,hyp.noise_theta,hyp.noise_sigma)\n",
    "    noise_factor = hyp.noise_init\n",
    "    noise_decay = hyp.noise_decay\n",
    "    \n",
    "    for episode in range(hyp.max_eps):\n",
    "        \n",
    "        noise_gen.reset()\n",
    "        n_step_buf = MultistepBuffer(hyp)\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = [0]*hyp.num_agnt\n",
    "        dones = [False]*hyp.num_agnt\n",
    "        \n",
    "        while dones.count(True) < hyp.num_agnt:\n",
    "            \n",
    "            in_states = torch.tensor(states,device=device).reshape((1,hyp.state_dim*hyp.num_agnt))\n",
    "            if not hyp.random_policy:\n",
    "                pol_net.eval()\n",
    "                actions = pol_net(in_states).reshape((hyp.num_agnt,hyp.act_dim)).cpu().detach()\n",
    "                pol_net.train()\n",
    "            else:\n",
    "                actions = torch.zeros((hyp.num_agnt,hyp.act_dim))\n",
    "            \n",
    "            act_noise = noise_factor*noise_gen.sample().reshape((hyp.num_agnt, hyp.act_dim))\n",
    "            actions = torch.clamp(torch.add(actions,act_noise),-1.0,1.0)\n",
    "            \n",
    "            env_info = env.step(actions.numpy())[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            env_dones = env_info.local_done\n",
    "            \n",
    "            tot_step_rwd = 0\n",
    "            for i in range(hyp.num_agnt):\n",
    "                \n",
    "                if dones[i] == False:\n",
    "                    \n",
    "                    scores[i] += rewards[i]\n",
    "                    tot_step_rwd += rewards[i]*(1.0 if rewards[i] >= 0 else hyp.rwd_mult_neg)\n",
    "                    dones[i] = env_dones[i]\n",
    "            \n",
    "            new_exp = Experience(in_states.cpu().detach(),\n",
    "                                 actions.reshape((1,hyp.num_agnt*hyp.act_dim)),\n",
    "                                 tot_step_rwd,\n",
    "                                 None)\n",
    "            n_step_buf.add_experience(new_exp)\n",
    "            \n",
    "            process_new_exp(hyp, device, out_exps, n_step_buf, replay_buffer, q_net, q_trg, pol_trg)\n",
    "                \n",
    "            # Handle rewards assigned at end of episodes.  Repeat state push into\n",
    "            # multistep buffer and add resulting n_step experience\n",
    "            if dones.count(True) >= hyp.num_agnt:\n",
    "                n_step_buf.add_experience(new_exp)\n",
    "                process_new_exp(hyp, device, out_exps, n_step_buf, \n",
    "                                replay_buffer, q_net, q_trg, pol_trg)\n",
    "                \n",
    "            states = next_states\n",
    "            \n",
    "        lrn_cntr += 1\n",
    "        if (lrn_cntr % hyp.lr_int) == 0:\n",
    "\n",
    "            lrn_cntr = 0\n",
    "            if len(replay_buffer) > max(hyp.buf_min_size, hyp.batch_size):\n",
    "\n",
    "                q_perf_log = PerformanceLogger()\n",
    "                pol_perf_log = PerformanceLogger()\n",
    "                for l_step in range(hyp.lr_stps):\n",
    "\n",
    "                    # Check that a sample op did not trigger reset\n",
    "                    if len(replay_buffer) < max(hyp.buf_min_size, hyp.batch_size):\n",
    "                        break\n",
    "\n",
    "                    samp_exp = replay_buffer.sample(hyp.batch_size)\n",
    "\n",
    "                    # Q Network Update\n",
    "                    q_optim.zero_grad()\n",
    "                    loss, w_i = Q_Loss(q_net, q_trg, pol_trg, samp_exp, hyp, device)\n",
    "                    tensor_check(loss,'q_loss',(hyp.batch_size,1))\n",
    "                    mean_loss = torch.mean(loss)\n",
    "                    mean_loss.backward()\n",
    "                    q_optim.step()\n",
    "                    q_perf_log.add_score(mean_loss.item())\n",
    "                    new_p_i = torch.div(loss,w_i).pow(hyp.alpha).squeeze(dim=1)\n",
    "\n",
    "                    pol_upd_cntr += 1\n",
    "                    if (pol_upd_cntr == hyp.pol_upd_int):\n",
    "                        pol_upd_cntr = 0\n",
    "                        # Policy Network Update\n",
    "                        q_optim.zero_grad()\n",
    "                        pol_optim.zero_grad()\n",
    "                        loss = Pol_Loss(q_net, pol_net, samp_exp, hyp, device)\n",
    "                        loss.backward()\n",
    "                        pol_optim.step()\n",
    "                        pol_perf_log.add_score(loss.item())\n",
    "\n",
    "                    # Update priorities in replay buffer according to losses\n",
    "                    for exp_num in range(hyp.batch_size):\n",
    "                        samp_exp[0][exp_num].update_p( \\\n",
    "                            (new_p_i[exp_num].item() - samp_exp[0][exp_num].p_i))\n",
    "\n",
    "                    # Update target networks\n",
    "                    soft_update(q_net, q_trg, hyp.q_tau)\n",
    "                    soft_update(pol_net, pol_trg, hyp.p_tau)\n",
    "\n",
    "                    # Output status of optimization\n",
    "                    q_avg = q_perf_log.run_avg()\n",
    "                    p_avg = pol_perf_log.run_avg()\n",
    "                    print(f'Completed {l_step + 1} of {hyp.lr_stps} learning steps. ' +\n",
    "                          f'Q Loss = {q_avg:.5f}, Policy Value = {-p_avg:.5f}', end='\\r')\n",
    "\n",
    "        perf_log.add_score(max(scores))\n",
    "        noise_factor = noise_factor * noise_decay\n",
    "        \n",
    "        if perf_log.has_full_window() and (perf_log.run_avg() >= hyp.slv_thresh):\n",
    "            print(f'Solved with average score of {perf_log.run_avg()} in {episode+1} episodes')\n",
    "            break\n",
    "        \n",
    "        rpt_cntr += 1\n",
    "        if (rpt_cntr % hyp.rprt_int) == 0:\n",
    "            rpt_cntr = 0\n",
    "            print(f'\\nCompleted {episode + 1} episodes. Average score = {perf_log.run_avg():.3f}')\n",
    "        \n",
    "        if episode == (hyp.max_eps - 1):\n",
    "            print(f'Failed to solve within the maximum of {hyp.max_eps} episodes')\n",
    "            break\n",
    "\n",
    "    # Save experience list to file\n",
    "    save_experiences(hyp, out_exps)\n",
    "    \n",
    "    return perf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83f325",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753b615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 100 episodes. Average score = 0.017\n",
      "\n",
      "Completed 200 episodes. Average score = 0.019\n",
      "Failed to solve within the maximum of 200 episodes\n",
      "Saved 3600 experiences to starter_exps.pkl\n"
     ]
    }
   ],
   "source": [
    "# First get some random experiences where a few balls are hit over, \n",
    "# for initializing other runs\n",
    "starter_hyp = DDPG_Hyperparameters(noise_init=5.0,\n",
    "                                   noise_decay=0.9999,\n",
    "                                   noise_theta=0.1,\n",
    "                                   noise_sigma=0.1,\n",
    "                                   n_step_order=1,\n",
    "                                   lr_int=1e6,\n",
    "                                   max_eps=200,\n",
    "                                   rwd_mult_neg=3.0,\n",
    "                                   output_exp_path='starter_exps.pkl',\n",
    "                                   random_policy=True)\n",
    "\n",
    "pol_net = new_pol_net(starter_hyp)\n",
    "q_net = new_q_net(starter_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, starter_hyp)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2176af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replay buffer created with initial size 3600\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00023, Policy Value = -0.04425\n",
      "Completed 100 episodes. Average score = 0.008\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00013, Policy Value = 0.014940\n",
      "Completed 200 episodes. Average score = 0.020\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00009, Policy Value = 0.02392\n",
      "Completed 300 episodes. Average score = 0.014\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00009, Policy Value = 0.02158\n",
      "Completed 400 episodes. Average score = 0.012\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00008, Policy Value = 0.01459\n",
      "Completed 500 episodes. Average score = 0.009\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00006, Policy Value = 0.00870\n",
      "Completed 600 episodes. Average score = 0.016\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00006, Policy Value = 0.00348\n",
      "Completed 700 episodes. Average score = 0.011\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00007, Policy Value = 0.00060\n",
      "Completed 800 episodes. Average score = 0.018\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00007, Policy Value = -0.00218\n",
      "Completed 900 episodes. Average score = 0.021\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00010, Policy Value = -0.00358\n",
      "Completed 1000 episodes. Average score = 0.024\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00010, Policy Value = -0.00582\n",
      "Completed 1100 episodes. Average score = 0.032\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00009, Policy Value = -0.00740\n",
      "Completed 1200 episodes. Average score = 0.027\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00009, Policy Value = -0.00844\n",
      "Completed 1300 episodes. Average score = 0.033\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00010, Policy Value = -0.00959\n",
      "Completed 1400 episodes. Average score = 0.039\n",
      "Completed 200 of 200 learning steps. Q Loss = 0.00012, Policy Value = -0.00975\n",
      "Completed 1500 episodes. Average score = 0.047\n",
      "Failed to solve within the maximum of 1500 episodes\n",
      "Saved 28595 experiences to serving_exps.pkl\n"
     ]
    }
   ],
   "source": [
    "# Run with relatively hot parameters.  \n",
    "# Get enough experiences to be able to learn to get consistent serves, \n",
    "# and avoid common pitfalls\n",
    "serving_hyp = DDPG_Hyperparameters(pol_hid_num=3,\n",
    "                                   pol_hid_size=512,\n",
    "                                   noise_init=5.0,\n",
    "                                   noise_decay=0.999,\n",
    "                                   noise_theta=0.1,\n",
    "                                   noise_sigma=0.5,\n",
    "                                   q_hid_num=3,\n",
    "                                   q_hid_size=512,\n",
    "                                   gamma=0.85,\n",
    "                                   alpha=0.0,\n",
    "                                   beta=1.0,\n",
    "                                   pol_lr=5e-5,\n",
    "                                   q_lr=5e-5,\n",
    "                                   buf_min_size=512,\n",
    "                                   lr_int=10,\n",
    "                                   lr_stps=200,\n",
    "                                   batch_size=512,\n",
    "                                   p_tau=1e-3,\n",
    "                                   q_tau=1e-3,\n",
    "                                   max_eps=1500,\n",
    "                                   rwd_mult_neg=3.0,\n",
    "                                   starting_exps_paths=['starter_exps.pkl'],\n",
    "                                   output_exp_path='serving_exps.pkl'\n",
    "                                  ) \n",
    "\n",
    "pol_net = new_pol_net(serving_hyp)\n",
    "q_net = new_q_net(serving_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, serving_hyp)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e339958c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replay buffer created with initial size 32195\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00235, Policy Value = 0.36411\n",
      "Completed 100 episodes. Average score = 0.003\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00083, Policy Value = 0.29299\n",
      "Completed 200 episodes. Average score = 0.003\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00047, Policy Value = 0.22619\n",
      "Completed 300 episodes. Average score = 0.001\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00031, Policy Value = 0.16928\n",
      "Completed 400 episodes. Average score = 0.019\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00024, Policy Value = 0.12284\n",
      "Completed 500 episodes. Average score = 0.017\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00021, Policy Value = 0.08604\n",
      "Completed 600 episodes. Average score = 0.014\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00019, Policy Value = 0.05781\n",
      "Completed 700 episodes. Average score = 0.024\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00018, Policy Value = 0.03603\n",
      "Completed 800 episodes. Average score = 0.018\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00017, Policy Value = 0.01888\n",
      "Completed 900 episodes. Average score = 0.018\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00016, Policy Value = 0.00638\n",
      "Completed 1000 episodes. Average score = 0.016\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00015, Policy Value = -0.00408\n",
      "Completed 1100 episodes. Average score = 0.014\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00015, Policy Value = -0.01164\n",
      "Completed 1200 episodes. Average score = 0.009\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00014, Policy Value = -0.01826\n",
      "Completed 1300 episodes. Average score = 0.012\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00014, Policy Value = -0.02270\n",
      "Completed 1400 episodes. Average score = 0.014\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00013, Policy Value = -0.02655\n",
      "Completed 1500 episodes. Average score = 0.027\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00014, Policy Value = -0.02188\n",
      "Completed 1600 episodes. Average score = 0.008\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00013, Policy Value = -0.02301\n",
      "Completed 1700 episodes. Average score = 0.001\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00013, Policy Value = -0.02621\n",
      "Completed 1800 episodes. Average score = 0.014\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00014, Policy Value = -0.02858\n",
      "Completed 1900 episodes. Average score = 0.019\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00014, Policy Value = -0.02818\n",
      "Completed 2000 episodes. Average score = 0.010\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00016, Policy Value = -0.02532\n",
      "Completed 2100 episodes. Average score = 0.035\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00016, Policy Value = -0.02396\n",
      "Completed 2200 episodes. Average score = 0.049\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00016, Policy Value = -0.02224\n",
      "Completed 2300 episodes. Average score = 0.037\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00018, Policy Value = -0.02114\n",
      "Completed 2400 episodes. Average score = 0.045\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00017, Policy Value = -0.02077\n",
      "Completed 2500 episodes. Average score = 0.042\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00017, Policy Value = -0.01972\n",
      "Completed 2600 episodes. Average score = 0.041\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00018, Policy Value = -0.01955\n",
      "Completed 2700 episodes. Average score = 0.037\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00019, Policy Value = -0.01933\n",
      "Completed 2800 episodes. Average score = 0.050\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00019, Policy Value = -0.01866\n",
      "Completed 2900 episodes. Average score = 0.052\n",
      "Completed 400 of 400 learning steps. Q Loss = 0.00019, Policy Value = -0.01829\n",
      "Completed 3000 episodes. Average score = 0.063\n",
      "Failed to solve within the maximum of 3000 episodes\n",
      "Saved 57886 experiences to cool_exps.pkl\n"
     ]
    }
   ],
   "source": [
    "# Run with relatively cool parameters using the previous experiences\n",
    "cool_hyp = DDPG_Hyperparameters(pol_hid_num=3,\n",
    "                                pol_hid_size=512,\n",
    "                                noise_init=2.5,\n",
    "                                noise_decay=0.999,\n",
    "                                noise_theta=0.33,\n",
    "                                noise_sigma=0.5,\n",
    "                                q_hid_num=3,\n",
    "                                q_hid_size=512,\n",
    "                                gamma=0.85,\n",
    "                                alpha=0.0,\n",
    "                                beta=1.0,\n",
    "                                pol_lr=5e-6,\n",
    "                                q_lr=5e-6,\n",
    "                                buf_min_size=512,\n",
    "                                lr_int=10,\n",
    "                                lr_stps=400,\n",
    "                                batch_size=512,\n",
    "                                p_tau=5e-4,\n",
    "                                q_tau=5e-4,\n",
    "                                max_eps=3000,\n",
    "                                rwd_mult_neg=3.0,\n",
    "                                starting_exps_paths=['starter_exps.pkl','serving_exps.pkl'],\n",
    "                                output_exp_path='cool_exps.pkl'\n",
    "                               ) \n",
    "\n",
    "\n",
    "pol_net = new_pol_net(cool_hyp)\n",
    "q_net = new_q_net(cool_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, cool_hyp)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72edd84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replay buffer created with initial size 90081\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00016, Policy Value = 0.09202\n",
      "Completed 100 episodes. Average score = 0.018\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00026, Policy Value = 0.04915\n",
      "Completed 200 episodes. Average score = 0.051\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00031, Policy Value = 0.04961\n",
      "Completed 300 episodes. Average score = 0.100\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00029, Policy Value = 0.06851\n",
      "Completed 400 episodes. Average score = 0.119\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00029, Policy Value = 0.09172\n",
      "Completed 500 episodes. Average score = 0.131\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00031, Policy Value = 0.11950\n",
      "Completed 600 episodes. Average score = 0.148\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00027, Policy Value = 0.14164\n",
      "Completed 700 episodes. Average score = 0.133\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00026, Policy Value = 0.16035\n",
      "Completed 800 episodes. Average score = 0.138\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00026, Policy Value = 0.18212\n",
      "Completed 900 episodes. Average score = 0.117\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00028, Policy Value = 0.19884\n",
      "Completed 1000 episodes. Average score = 0.133\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00025, Policy Value = 0.21075\n",
      "Completed 1100 episodes. Average score = 0.122\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00025, Policy Value = 0.22415\n",
      "Completed 1200 episodes. Average score = 0.138\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00024, Policy Value = 0.23642\n",
      "Completed 1300 episodes. Average score = 0.142\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00025, Policy Value = 0.24881\n",
      "Completed 1400 episodes. Average score = 0.128\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00023, Policy Value = 0.25628\n",
      "Completed 1500 episodes. Average score = 0.154\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00025, Policy Value = 0.26683\n",
      "Completed 1600 episodes. Average score = 0.181\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00024, Policy Value = 0.27427\n",
      "Completed 1700 episodes. Average score = 0.139\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00023, Policy Value = 0.27902\n",
      "Completed 1800 episodes. Average score = 0.123\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00026, Policy Value = 0.29098\n",
      "Completed 1900 episodes. Average score = 0.137\n",
      "Completed 8000 of 8000 learning steps. Q Loss = 0.00027, Policy Value = 0.31034\n",
      "Completed 2000 episodes. Average score = 0.109\n",
      "Failed to solve within the maximum of 2000 episodes\n",
      "Saved 98983 experiences to run_four_exps.pkl\n"
     ]
    }
   ],
   "source": [
    "# Try higher gamma, to make how easy it is to return a hit ball have more\n",
    "# influence over how ball is hit.  Also added a layer to each network and\n",
    "# increased learning rates in case improvement stall due to local minima\n",
    "# residence\n",
    "four_hyp = DDPG_Hyperparameters(pol_hid_num=4,\n",
    "                                pol_hid_size=512,\n",
    "                                noise_init=1.25,\n",
    "                                noise_decay=0.99,\n",
    "                                noise_theta=0.33,\n",
    "                                noise_sigma=0.5,\n",
    "                                q_hid_num=4,\n",
    "                                q_hid_size=512,\n",
    "                                gamma=0.95,\n",
    "                                alpha=0.0,\n",
    "                                beta=1.0,\n",
    "                                pol_lr=1e-4,\n",
    "                                q_lr=1e-4,\n",
    "                                buf_min_size=512,\n",
    "                                lr_int=100,\n",
    "                                lr_stps=8000,\n",
    "                                batch_size=512,\n",
    "                                p_tau=1e-3,\n",
    "                                q_tau=1e-3,\n",
    "                                max_eps=2000,\n",
    "                                rwd_mult_neg=3.0,\n",
    "                                starting_exps_paths=['starter_exps.pkl',\n",
    "                                                     'serving_exps.pkl',\n",
    "                                                     'cool_exps.pkl'],\n",
    "                                output_exp_path='run_four_exps.pkl'\n",
    "                               ) \n",
    "\n",
    "\n",
    "pol_net = new_pol_net(four_hyp)\n",
    "q_net = new_q_net(four_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, four_hyp)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd62e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replay buffer created with initial size 189064\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00052, Policy Value = -0.02681\n",
      "Completed 100 episodes. Average score = 0.021\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00052, Policy Value = 0.03642\n",
      "Completed 200 episodes. Average score = 0.216\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00055, Policy Value = 0.08074\n",
      "Completed 300 episodes. Average score = 0.196\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00060, Policy Value = 0.12156\n",
      "Completed 400 episodes. Average score = 0.252\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00059, Policy Value = 0.16710\n",
      "Completed 500 episodes. Average score = 0.302\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00063, Policy Value = 0.21070\n",
      "Completed 600 episodes. Average score = 0.219\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00063, Policy Value = 0.24868\n",
      "Completed 700 episodes. Average score = 0.156\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00066, Policy Value = 0.28266\n",
      "Completed 800 episodes. Average score = 0.140\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00072, Policy Value = 0.31772\n",
      "Completed 900 episodes. Average score = 0.152\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00079, Policy Value = 0.35834\n",
      "Completed 1000 episodes. Average score = 0.146\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00122, Policy Value = 0.45064\n",
      "Completed 1100 episodes. Average score = 0.127\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00205, Policy Value = 0.64825\n",
      "Completed 1200 episodes. Average score = 0.086\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00256, Policy Value = 0.91125\n",
      "Completed 1300 episodes. Average score = 0.020\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00274, Policy Value = 1.10249\n",
      "Completed 1400 episodes. Average score = 0.016\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00361, Policy Value = 1.35459\n",
      "Completed 1500 episodes. Average score = 0.005\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00499, Policy Value = 1.75947\n",
      "Completed 1600 episodes. Average score = 0.001\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00713, Policy Value = 2.25986\n",
      "Completed 1700 episodes. Average score = 0.001\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00953, Policy Value = 2.88738\n",
      "Completed 1800 episodes. Average score = 0.000\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.01192, Policy Value = 3.57006\n",
      "Completed 1900 episodes. Average score = 0.004\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.02132, Policy Value = 4.60220\n",
      "Completed 2000 episodes. Average score = 0.003\n",
      "Failed to solve within the maximum of 2000 episodes\n",
      "Saved 94665 experiences to run_five_exps.pkl\n"
     ]
    }
   ],
   "source": [
    "# Still some difficulty past the first / second hit.\n",
    "# Lower discount rate (higher gamma).  Lower learning rates\n",
    "# by factor of two as attempt to avoid divergence\n",
    "five_hyp = DDPG_Hyperparameters(pol_hid_num=4,\n",
    "                                pol_hid_size=512,\n",
    "                                noise_init=1.25,\n",
    "                                noise_decay=0.995,\n",
    "                                noise_theta=0.33,\n",
    "                                noise_sigma=0.5,\n",
    "                                q_hid_num=4,\n",
    "                                q_hid_size=512,\n",
    "                                gamma=0.96,\n",
    "                                alpha=0.0,\n",
    "                                beta=1.0,\n",
    "                                pol_lr=5e-5,\n",
    "                                q_lr=5e-5,\n",
    "                                buf_min_size=512,\n",
    "                                lr_int=100,\n",
    "                                lr_stps=16000,\n",
    "                                batch_size=512,\n",
    "                                p_tau=1e-3,\n",
    "                                q_tau=1e-3,\n",
    "                                max_eps=2000,\n",
    "                                rwd_mult_neg=3.0,\n",
    "                                starting_exps_paths=['starter_exps.pkl',\n",
    "                                                     'serving_exps.pkl',\n",
    "                                                     'cool_exps.pkl',\n",
    "                                                     'run_four_exps.pkl'],\n",
    "                                output_exp_path='run_five_exps.pkl'\n",
    "                               ) \n",
    "\n",
    "\n",
    "pol_net = new_pol_net(five_hyp)\n",
    "q_net = new_q_net(five_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, five_hyp)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96be65a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replay buffer created with initial size 283729\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00029, Policy Value = 0.06110\n",
      "Completed 100 episodes. Average score = 0.018\n",
      "Completed 16000 of 16000 learning steps. Q Loss = 0.00033, Policy Value = 0.03452\n",
      "Completed 200 episodes. Average score = 0.049\n",
      "Solved with average score of 0.5210000078938898 in 263 episodes\n",
      "Saved 23387 experiences to run_six_exps.pkl\n"
     ]
    }
   ],
   "source": [
    "# Looks like got numerically unstable after peaking, around episode 500.\n",
    "# Lower gamma some.  Also set steps per learning phase real high since\n",
    "# should have a lot to work from without an immediate need for more\n",
    "# experiences.\n",
    "six_hyp = DDPG_Hyperparameters(pol_hid_num=4,\n",
    "                                pol_hid_size=512,\n",
    "                                noise_init=1.25,\n",
    "                                noise_decay=0.995,\n",
    "                                noise_theta=0.33,\n",
    "                                noise_sigma=0.5,\n",
    "                                q_hid_num=4,\n",
    "                                q_hid_size=512,\n",
    "                                gamma=0.93,\n",
    "                                alpha=0.0,\n",
    "                                beta=1.0,\n",
    "                                pol_lr=5e-5,\n",
    "                                q_lr=5e-5,\n",
    "                                buf_min_size=512,\n",
    "                                lr_int=100,\n",
    "                                lr_stps=16000,\n",
    "                                batch_size=512,\n",
    "                                p_tau=1e-3,\n",
    "                                q_tau=1e-3,\n",
    "                                max_eps=2000,\n",
    "                                rwd_mult_neg=3.0,\n",
    "                                starting_exps_paths=['starter_exps.pkl',\n",
    "                                                     'serving_exps.pkl',\n",
    "                                                     'cool_exps.pkl',\n",
    "                                                     'run_four_exps.pkl',\n",
    "                                                     'run_five_exps.pkl'],\n",
    "                                output_exp_path='run_six_exps.pkl'\n",
    "                               ) \n",
    "\n",
    "\n",
    "pol_net = new_pol_net(six_hyp)\n",
    "q_net = new_q_net(six_hyp)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(pol_net, q_net, env, six_hyp)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd0dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final models\n",
    "torch.save(pol_net,f'pol_net_final.pt')\n",
    "torch.save(q_net,'q_net_final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feae780",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d84f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rslt.scores)\n",
    "plt.title('Training Results')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf5740",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Quite a lot of runs were required to determine hyperparameters that resulted in stable learning.<br>\n",
    "The persistent storage of experiences, and their recovery before starting training runs, were implemented in anticipation of far more iterations being required.  However, this feature seemed to result in success relatively quickly.  Learning with this method may have not only made training take less time, but improved performance.  This is because, by the time an agent starts making a certain class of mistakes, it has also become habituated to making those mistakes (local minima).  Starting fresh with a new neural network, that's able to access the failed experiences of previous runs, is one way of escaping the local minima while retaining the knowledge obtained there.\n",
    "\n",
    "Note from the training plot that the persistent experiences from previous runs were almost entirely responsible for success, as 250 episodes is very small relative to the benchmark plot Udacity provided, which demonstrated a requirement of 1500 episodes to solve the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcc45c",
   "metadata": {},
   "source": [
    "### Demo\n",
    "The cell below can be run to demonstrate the behavior of the trained agent. It is necessary to first run all cells except those in the training section. One can also review this [video](https://youtu.be/XOyMPkYiyz4) of the trained agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fd44f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 1 episodes. Average score = 0.100\n",
      "\n",
      "Completed 2 episodes. Average score = 0.150\n",
      "\n",
      "Completed 3 episodes. Average score = 0.130\n",
      "\n",
      "Completed 4 episodes. Average score = 0.173\n",
      "\n",
      "Completed 5 episodes. Average score = 0.658\n",
      "Failed to solve within the maximum of 5 episodes\n",
      "Saved 1237 experiences to exp_output.pkl\n"
     ]
    }
   ],
   "source": [
    "demo_eps = 5\n",
    "\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "demo_pol_net = torch.load('submission_pol_net.pt')\n",
    "demo_q_net = torch.load('submission_q_net.pt')\n",
    "demo_hyp = DDPG_Hyperparameters(noise_init=0.0,\n",
    "                                pol_hid_num=4,\n",
    "                                pol_hid_size=512,\n",
    "                                q_hid_num=4,\n",
    "                                q_hid_size=512,\n",
    "                                max_eps=demo_eps,\n",
    "                                lr_int=(demo_eps*1e9),\n",
    "                                rprt_int=1,\n",
    "                                slv_thresh=1e9\n",
    "                               )\n",
    "\n",
    "train_net(demo_pol_net,demo_q_net,env,demo_hyp)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656ce7d",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "\n",
    "+ The agent is fairly brittle as evidenced by the results plot.  Either the agent drops the ball quickly, or settles into a stable routine from which the ball never drops.  Training a single-sided agent against a random policy opponent might eventually result in a more robust agent.\n",
    "+ The persistent experience storage mechanism was very helpful, and seems like it should be implemented with any reinforcement learning problem before training starts.  A more flexible library implementation would be valuable.\n",
    "+ Apply the algorithm to more current Unity environments could aid in development of intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaa731",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Lillicrap et. al., Continuous control with deep reinforcement learning, [arXiv:1509.02971](https://arxiv.org/abs/1509.02971) <br>\n",
    "[2] Schaul et. al., Prioritized Experience Replay, [arXiv:1511.05952](https://arxiv.org/abs/1511.05952)<br>\n",
    "[3] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
